# Analyse des Donn√©es YouTube ‚Äì Technique de Programmation
## Contexte du projet
√Ä l‚Äô√®re des plateformes num√©riques, YouTube se pr√©sente comme un service capable d‚Äôoffrir une exp√©rience de consommation de contenu enti√®rement personnalis√©e, suppos√©ment align√©e sur les pr√©f√©rences individuelles de chaque utilisateur. Cette promesse de personnalisation s‚Äôaccompagne toutefois d‚Äôune transformation profonde de l‚Äôinterface et des modes d‚Äôacc√®s √† l‚Äôinformation.

L‚Äôonglet ¬´ Tendances ¬ª, autrefois commun √† l‚Äôensemble des utilisateurs et garant d‚Äôune certaine transparence collective, a progressivement √©t√© marginalis√©, voire remplac√©, par un flux algorithmique individualis√©. Cette √©volution rend de moins en moins perceptible l‚Äôexistence de dynamiques globales de diffusion de contenus. L‚Äôutilisateur a alors le sentiment de naviguer librement dans un environnement riche et diversifi√©, alors m√™me que ses choix sont en partie orient√©s par des m√©canismes algorithmiques invisibles.

Ces m√©canismes, con√ßus pour maximiser le temps d‚Äôengagement et la r√©tention de l‚Äôattention, soul√®vent des enjeux majeurs en mati√®re de sant√© mentale, de bien-√™tre psychologique et de libert√© de choix. Le paradoxe r√©side dans le fait que plus l‚Äôalgorithme se pr√©tend ¬´ personnel ¬ª, plus son fonctionnement r√©el √©chappe √† la compr√©hension de l‚Äôutilisateur.

## Objectifs
Ce projet a pour objectif de rendre visible l‚Äôinfluence des recommandations algorithmiques de YouTube √† partir des donn√©es personnelles de l‚Äôutilisateur. En exploitant les donn√©es YouTube Takeout et la programmation Python, il vise √† :
- Analyser l‚Äôhistorique de visionnage de mani√®re objective.
- Identifier les th√©matiques et tendances dominantes dans les contenus recommand√©s.
- √âvaluer le degr√© d‚Äôexposition √† des dynamiques potentiellement probl√©matiques.
- Favoriser une prise de recul critique face aux m√©canismes de personnalisation.

Le projet s‚Äôinscrit dans une d√©marche d‚Äôaudit algorithmique et d‚Äôhygi√®ne num√©rique, permettant √† l‚Äôutilisateur de mieux comprendre son environnement informationnel.

## Technologies utilis√©es
- Python
- Fichiers JSON
- Pandas : manipulation des donn√©es
- YouTube Data API v3
- Jupyter Notebook (pour la restitution et la visualisation)

##  Structure du projet
- Installation et configuration initiale
- Analyse comportementale profonde
- Audit d'influence directe (Score de Sensibilit√©)
- Calcul du Score de Sensibilit√© (1-10)
- Analyse s√©mantique des tags (Influence Th√©matique)
- Conclusion

## Description technique du script
### 1.Cr√©ation et chargement de la base de donn√©es locale
- Description technique

Ce premier bloc met en place l‚Äôenvironnement logiciel du projet.
Il importe les biblioth√®ques essentielles : Pandas pour la manipulation des donn√©es, Matplotlib / Seaborn pour la visualisation, et google-api-python-client pour l‚Äôacc√®s √† l‚ÄôAPI YouTube.
La cl√© API permet l‚Äôauthentification des requ√™tes et l‚Äôacc√®s aux m√©tadonn√©es normalement non expos√©es √† l‚Äôutilisateur.

```
# Cellule 1 : Installation des outils
!pip install pandas matplotlib seaborn google-api-python-client isodate
```

- Interpr√©tation analytique

Cette √©tape constitue le socle de l‚Äôappropriation des donn√©es personnelles.
En configurant ces outils, l‚Äôutilisateur ne se contente plus de consommer une interface, mais se dote des moyens techniques pour analyser le fonctionnement algorithmique sous-jacent.
L‚Äôusage de l‚ÄôAPI YouTube est central : il donne acc√®s √† des informations structurelles (classements, cat√©gories, m√©tadonn√©es) absentes de l‚Äôinterface publique, rendant possible un audit ind√©pendant du syst√®me de recommandation.

### 2.D√©tection des nouvelles vid√©os

- Description technique

Ce code charge le fichier watch-history.json issu de Google Takeout et enrichit chaque vid√©o √† l‚Äôaide de l‚ÄôAPI YouTube (cat√©gorie, dur√©e, vues, tags).
Un m√©canisme de mise √† jour incr√©mentale est impl√©ment√© afin de limiter la consommation du quota API : seules les vid√©os non encore analys√©es sont trait√©es.
Les visualisations produites portent sur les cha√Ænes dominantes, la r√©partition horaire du visionnage et la popularit√© des contenus consomm√©s.

```
import pandas as pd
import json
import os
from googleapiclient.discovery import build
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime


# ==========================================
# MODULE 1 : GESTION DES DONN√âES ET MISE √Ä JOUR
# ==========================================

def load_takeout_data(filepath):
    """Charge et nettoie les donn√©es brutes de Google Takeout."""
    print(f"Chargement de {filepath}...")
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print("Erreur : Fichier Takeout introuvable.")
        return pd.DataFrame()

    clean_data = []
    for entry in data:
        # On ne garde que les entr√©es qui sont des vid√©os (pas les pubs ou visites de chaine)
        if 'titleUrl' in entry and 'watch?v=' in entry['titleUrl']:
            video_id = entry['titleUrl'].split('watch?v=')[1]
            # Gestion de la date
            watch_time = entry['time'] # Format ISO
            clean_data.append({
                'video_id': video_id,
                'title': entry['title'].replace("Vous avez regard√© ", ""),
                'watch_time': watch_time
            })

    return pd.DataFrame(clean_data)

def get_video_details(video_ids, api_key):
    """Interroge l'API YouTube pour r√©cup√©rer les cat√©gories (Enrichissement)."""
    youtube = build('youtube', 'v3', developerKey=api_key)
    video_details = []

    # L'API accepte max 50 IDs par requ√™te
    chunk_size = 50
    for i in range(0, len(video_ids), chunk_size):
        chunk = video_ids[i:i+chunk_size]
        request = youtube.videos().list(
            part="snippet,contentDetails,statistics",
            id=','.join(chunk)
        )
        response = request.execute()

        for item in response['items']:
            video_details.append({
                'video_id': item['id'],
                'category_id': item['snippet']['categoryId'],
                'channel_title': item['snippet']['channelTitle'],
                'tags': item['snippet'].get('tags', []), # Les tags sont cl√©s pour comprendre les tendances
                'duration': item['contentDetails']['duration'],
                'view_count': item['statistics'].get('viewCount', 0)
            })

    return pd.DataFrame(video_details)

def update_database(new_df):
    """M√©canisme de mise √† jour incr√©mentale."""
    # 1. Charger la base existante si elle existe
    if os.path.exists(DB_FILE):
        existing_db = pd.read_csv(DB_FILE)
        print(f"Base existante charg√©e : {len(existing_db)} vid√©os.")
    else:
        existing_db = pd.DataFrame(columns=['video_id'])
        print("Cr√©ation d'une nouvelle base de donn√©es locale.")

    # 2. Identifier les vid√©os inconnues (qu'on n'a pas encore analys√©es)
    # On compare les IDs du Takeout avec ceux de la DB locale
    merged = new_df.merge(existing_db, on='video_id', how='left', indicator=True)
    unknown_videos = merged[merged['_merge'] == 'left_only']['video_id'].unique()

    print(f"Nouvelles vid√©os d√©tect√©es √† analyser via API : {len(unknown_videos)}")

    if len(unknown_videos) > 0:
        # Attention aux quotas API (limitez si n√©cessaire pour les tests)
        # Pour l'exemple, on limite √† 200 nouvelles vid√©os pour ne pas griller le quota gratuit
        details_df = get_video_details(list(unknown_videos)[:200], API_KEY)

        # Fusionner et sauvegarder
        full_db = pd.concat([existing_db, details_df], ignore_index=True).drop_duplicates(subset=['video_id'])
        full_db.to_csv(DB_FILE, index=False)
        print("Base de donn√©es mise √† jour avec succ√®s.")
        return full_db
    else:
        print("Aucune nouvelle donn√©e API n√©cessaire.")
        return existing_db

# ==========================================
# MODULE 2 : ANALYSE ET VISUALISATION
# ==========================================

def analyze_trends(history_df, metadata_df):
    """Cr√©e les graphiques pour la pr√©sentation."""

    # Fusionner l'historique (QUAND j'ai regard√©) avec les m√©tadonn√©es (QUOI)
    df = history_df.merge(metadata_df, on='video_id', how='inner')

    # Convertir le temps
    df['watch_time'] = pd.to_datetime(df['watch_time'], format='ISO8601') # Added format='ISO8601'
    df['hour'] = df['watch_time'].dt.hour
    df['day_of_week'] = df['watch_time'].dt.day_name()

    # Configuration du style
    sns.set_theme(style="whitegrid")

    # --- Graphique 1 : Ma "Bulle" (Cat√©gories) ---
    # Note: L'API renvoie des ID de cat√©gories (ex: '10' = Music).
    # Il faudrait une map compl√®te, ici simplifi√©.
    plt.figure(figsize=(10, 6))
    top_channels = df['channel_title'].value_counts().head(10)
    sns.barplot(x=top_channels.values, y=top_channels.index, palette="viridis")
    plt.title("Les 10 cha√Ænes qui dominent mon flux (Ma Bulle)")
    plt.xlabel("Nombre de vues")
    plt.tight_layout()
    plt.show()

    # --- Graphique 2 : Chronobiologie de l'addiction ---
    plt.figure(figsize=(10, 6))
    sns.histplot(df['hour'], bins=24, kde=True, color="red")
    plt.title("√Ä quelle heure l'algorithme me capte-t-il le mieux ?")
    plt.xlabel("Heure de la journ√©e (0-24)")
    plt.ylabel("Fr√©quence")
    plt.tight_layout()
    plt.show()

    # --- Graphique 3 : Mainstream vs Niche ---
    # Analyse bas√©e sur le nombre de vues global des vid√©os regard√©es
    df['view_count'] = pd.to_numeric(df['view_count'])

    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df['view_count'])
    plt.xscale('log') # √âchelle logarithmique car les vues varient √©norm√©ment
    plt.title("Distribution des vues : Suis-je un consommateur de contenu 'Viral' ?")
    plt.xlabel("Vues totales de la vid√©o (√âchelle Log)")
    plt.tight_layout()
    plt.show()

# ==========================================
# MAIN
# ==========================================

if __name__ == "__main__":
    # 1. Charger l'historique brut
    history = load_takeout_data(TAKEOUT_FILE)

    if not history.empty:
        # 2. Mettre √† jour les m√©tadonn√©es (API)
        metadata = update_database(history)

        # 3. Lancer l'analyse
        analyze_trends(history, metadata)
    else:
        print("Veuillez placer le fichier watch-history.json dans le dossier.")
```

![](Ma_Bulle.png)

![](Chronobiologie_de_l'addiction.png)

![](MainstreamvsNiche.png)

- Interpr√©tation analytique

Cette analyse met en √©vidence les r√©gularit√©s comportementales de consommation.
La distribution horaire r√©v√®le des p√©riodes de forte exposition algorithmique, tandis que la concentration sur un nombre restreint de cha√Ænes mat√©rialise l‚Äôexistence d‚Äôune bulle de filtrage.
L‚Äôanalyse du niveau de popularit√© (contenus mainstream vs niche) permet d‚Äô√©valuer le degr√© d‚Äôexposition aux logiques virales et aux m√©canismes de comparaison sociale associ√©s.

### 3.R√©cup√©ration des donn√©es via l‚ÄôAPI YouTube

- Description technique

Ce bloc r√©cup√®re le Top 50 des vid√©os tendances en France via l‚ÄôAPI YouTube et calcule leur intersection avec l‚Äôhistorique personnel de l‚Äôutilisateur.
Il g√©n√®re des diagrammes circulaires comparant les cat√©gories de contenus tendances et celles effectivement consomm√©es.
La structure CATEGORY_MAP permet de traduire les identifiants num√©riques en cat√©gories lisibles.

```
import pandas as pd
import json
from googleapiclient.discovery import build
from tabulate import tabulate
import matplotlib.pyplot as plt
import seaborn as sns

# Liste de correspondance des ID et noms de cat√©gories (les plus courants)
CATEGORY_MAP = {
    "1": "Film & Animation",
    "2": "Autos & Vehicles",
    "10": "Music",
    "15": "Pets & Animals",
    "17": "Sports",
    "19": "Travel & Events",
    "20": "Gaming",
    "22": "People & Blogs",
    "23": "Comedy",
    "24": "Entertainment",
    "25": "News & Politics",
    "26": "Howto & Style",
    "27": "Education",
    "28": "Science & Technology",
    "29": "Nonprofits & Activism",
    "30": "Movies",
    "43": "Shows"
}

# ==========================================
# NOUVELLE FONCTION : AFFICHAGE DES CAT√âGORIES
# ==========================================

def display_category_table():
    """Affiche le tableau des ID et des noms pour la r√©f√©rence."""
    print("\n--- TABLEAU DE R√âF√âRENCE DES CAT√âGORIES YOUTUBE ---")
    table_data = [[id, name] for id, name in CATEGORY_MAP.items()]
    # Utilise tabulate pour un affichage propre
    print(tabulate(table_data, headers=["ID", "Cat√©gorie"], tablefmt="fancy_grid"))

def get_category_name(category_id):
    """Convertit un ID en nom de cat√©gorie."""
    return CATEGORY_MAP.get(category_id, f"ID Inconnu ({category_id})")

# ==========================================
# FONCTIONS D'ANALYSE (Reprise de la version pr√©c√©dente)
# ==========================================

def get_youtube_client():
    return build('youtube', 'v3', developerKey=API_KEY)

def get_current_trending_videos(youtube):
    """R√©cup√®re les tendances actuelles."""
    request = youtube.videos().list(
        part="snippet,statistics",
        chart="mostPopular",
        regionCode="FR",
        maxResults=50
    )
    response = request.execute()

    trending_list = []
    for item in response['items']:
        trending_list.append({
            'video_id': item['id'],
            'category_id': item['snippet']['categoryId']
        })
    return pd.DataFrame(trending_list)

def load_my_history(filepath):
    """Charge et nettoie l'historique."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"Erreur : Fichier {filepath} introuvable.")
        return pd.DataFrame()

    my_history = []
    for entry in data:
        if 'titleUrl' in entry and 'watch?v=' in entry['titleUrl']:
            video_id = entry['titleUrl'].split('watch?v=')[-1][:11]
            my_history.append({'video_id': video_id, 'date': entry['time']})
    return pd.DataFrame(my_history).drop_duplicates(subset=['video_id'])

def analyze_influence():
    youtube = get_youtube_client()

    df_trending = get_current_trending_videos(youtube)
    df_my_history = load_my_history(TAKEOUT_FILE)

    # √âtape 1 : Calculer l'intersection
    videos_communes = df_my_history[df_my_history['video_id'].isin(df_trending['video_id'])]
    nb_communs = len(videos_communes)

    # √âtape 2 : R√©cup√©rer les cat√©gories de votre historique r√©cent
    recent_ids = df_my_history['video_id'].head(50).tolist()
    res = youtube.videos().list(part="snippet", id=",".join(recent_ids)).execute()
    my_categories_list = [item['snippet']['categoryId'] for item in res['items']]

    df_my_cats = pd.DataFrame(my_categories_list, columns=['category_id'])

    # --- Pr√©paration pour le graphique ---
    df_trending['category_name'] = df_trending['category_id'].apply(get_category_name)
    df_my_cats['category_name'] = df_my_cats['category_id'].apply(get_category_name)

    # VISUALISATION
    sns.set_theme(style="whitegrid")

    plt.figure(figsize=(14, 7))

    # Graphique 1 : Cat√©gories en Tendance (Noms affich√©s)
    plt.subplot(1, 2, 1)
    # Utilisez 'category_name' pour les √©tiquettes
    df_trending['category_name'].value_counts().plot(kind='pie', autopct='%1.1f%%', title="Cat√©gories en Tendance (France)")
    plt.ylabel('Proportion')

    # Graphique 2 : Mes Cat√©gories (Noms affich√©s)
    plt.subplot(1, 2, 2)
    df_my_cats['category_name'].value_counts().plot(kind='pie', autopct='%1.1f%%', title="Mes Cat√©gories (Audit Personnel)")
    plt.ylabel('Proportion')

    plt.tight_layout()
    plt.show()

    # CONCLUSION
    print(f"\n--- R√âSULTAT DE L'AUDIT ---")
    print(f"Vous avez regard√© {nb_communs} vid√©os qui sont actuellement dans le Top 50 Tendances.")
    influence_score = (nb_communs / 50) * 100
    print(f"Votre indice d'influence directe est de : {influence_score:.1f}%")

# ==========================================
# MAIN EXECUTION
# ==========================================

if __name__ == "__main__":
    display_category_table() # Affiche le tableau de r√©f√©rence au d√©but
    analyze_influence()
```


![](CateÃÅgories_en_Tendance.png)


- Interpr√©tation analytique

Cette √©tape r√©pond √† la question centrale de l‚Äôinfluence visible des tendances.
Un taux d‚Äôintersection nul indique l‚Äôabsence de consommation directe des vid√©os tendances, non pas comme signe d‚Äôind√©pendance, mais comme indicateur d‚Äôune hyper-personnalisation algorithmique.
L‚Äôutilisateur est isol√© du flux grand public au profit d‚Äôun environnement parfaitement ajust√© √† son profil, r√©duisant la diversit√© cognitive et la s√©rendipit√©.

### 4.Fusion et mise √† jour de la base
- Description technique

Ce code synth√©tise les r√©sultats pr√©c√©dents en un indicateur unique.
Le pourcentage d‚Äôinfluence directe est converti en score sur 10, puis interpr√©t√© automatiquement selon trois niveaux :
Bulle de Niche (‚â§2), Influence Mod√©r√©e (3‚Äì5), Mainstream (‚â•6) .

```
import pandas as pd
import json
from googleapiclient.discovery import build
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# FONCTIONS TECHNIQUES
# ==========================================

def calculate_sensitivity_score(influence_percentage):
    """Calcule le score de 1 √† 10."""
    score = influence_percentage / 5
    if score < 1: score = 1.0
    if score > 10: score = 10.0
    return round(score, 1)

def get_current_trending_videos(youtube):
    """R√©cup√®re le Top 50 YouTube France."""
    try:
        request = youtube.videos().list(
            part="snippet,statistics",
            chart="mostPopular",
            regionCode="FR",
            maxResults=50
        )
        response = request.execute()
        trending_list = [{'video_id': item['id'], 'category_id': item['snippet']['categoryId']} for item in response['items']]
        return pd.DataFrame(trending_list)
    except Exception as e:
        print(f"Erreur API YouTube : {e}")
        return pd.DataFrame()

def load_my_history(filepath):
    """Charge votre fichier Takeout."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        my_history = []
        for entry in data:
            if 'titleUrl' in entry:
                video_id = entry['titleUrl'].split('v=')[-1][:11]
                my_history.append({'video_id': video_id})
        return pd.DataFrame(my_history)
    except FileNotFoundError:
        print(f"‚ùå ERREUR : Le fichier '{filepath}' est introuvable. Glissez-le dans le dossier √† gauche sur Colab.")
        return pd.DataFrame()

# ==========================================
# FONCTION PRINCIPALE (Celle qui affiche tout)
# ==========================================

def analyze_influence():
    print("üöÄ D√©marrage de l'analyse...\n")

    # 1. Connexion API
    youtube = build('youtube', 'v3', developerKey=API_KEY)

    # 2. Chargement des donn√©es
    df_trending = get_current_trending_videos(youtube)
    df_my_history = load_my_history(TAKEOUT_FILE)

    if df_trending.empty or df_my_history.empty:
        print("‚ö†Ô∏è Analyse impossible : donn√©es manquantes.")
        return

    # 3. Comparaison (Intersection)
    videos_communes = df_my_history[df_my_history['video_id'].isin(df_trending['video_id'])]
    nb_communs = len(videos_communes)

    # 4. Calcul des scores
    influence_perc = (nb_communs / 50) * 100
    sensi_score = calculate_sensitivity_score(influence_perc)

    # 5. AFFICHAGE DES R√âSULTATS DANS LA CONSOLE
    print("-" * 40)
    print("üìä R√âSULTATS DE L'AUDIT RGPD")
    print("-" * 40)
    print(f"‚úÖ Vid√©os analys√©es dans votre historique : {len(df_my_history)}")
    print(f"üî• Vid√©os en commun avec le Top 50 Tendances : {nb_communs}")
    print(f"üéØ Indice d'influence directe : {influence_perc}%")
    print(f"\n‚ö° SCORE DE SENSIBILIT√â (1-10) : {sensi_score}/10")
    print("-" * 40)

    # 6. Petit message d'interpr√©tation automatique
    if sensi_score <= 2:
        print("Interpr√©tation : Vous √™tes dans une 'Bulle de Niche'. L'algorithme vous conna√Æt trop bien pour vous proposer du contenu g√©n√©raliste.")
    elif sensi_score <= 5:
        print("Interpr√©tation : Influence mod√©r√©e. Vous suivez quelques tendances mais gardez un profil sp√©cifique.")
    else:
        print("Interpr√©tation : Profil 'Mainstream'. Vous √™tes fortement synchronis√© avec la culture populaire actuelle.")

# ==========================================
# LANCEMENT (Indispensable pour que √ßa s'affiche !)
# ==========================================
if __name__ == "__main__":
    analyze_influence()
```

- Interpr√©tation analytique

Un score faible traduit une pr√©dictibilit√© √©lev√©e du comportement utilisateur.
L‚Äôalgorithme n‚Äôa plus besoin d‚Äôexposer ce profil aux tendances globales, car son engagement est d√©j√† optimis√©.
Cette situation correspond √† une forme d‚Äôenfermement algorithmique : la personnalisation extr√™me maximise l‚Äôefficacit√© √©conomique du syst√®me au d√©triment de la diversit√© informationnelle.

### 5.Analyse des tags

- Description technique

Ce dernier bloc extrait les tags des vid√©os tendances et des vid√©os r√©cemment visionn√©es par l‚Äôutilisateur.
Il calcule l‚Äôintersection s√©mantique entre ces ensembles afin de mesurer une synchronisation th√©matique, ind√©pendamment de l‚Äôidentit√© exacte des vid√©os.

```
import pandas as pd
from googleapiclient.discovery import build
import matplotlib.pyplot as plt


def get_tags_analysis():
    youtube = build('youtube', 'v3', developerKey=API_KEY)

    # 1. R√âCUP√âRER LES TAGS DES TENDANCES (L'air du temps)
    print("Analyse des th√©matiques tendances en France...")
    request = youtube.videos().list(part="snippet", chart="mostPopular", regionCode="FR", maxResults=50)
    res_trending = request.execute()

    trending_tags = []
    for item in res_trending['items']:
        tags = item['snippet'].get('tags', [])
        trending_tags.extend([t.lower() for t in tags])

    # 2. R√âCUP√âRER LES TAGS DE VOTRE HISTORIQUE (Votre bulle)
    print("Analyse de vos th√©matiques personnelles...")
    # On prend les 20 derni√®res vid√©os pour voir l'influence r√©cente
    df_history = load_my_history(TAKEOUT_FILE).head(20)
    my_ids = df_history['video_id'].tolist()

    res_my_videos = youtube.videos().list(part="snippet", id=",".join(my_ids)).execute()
    my_tags = []
    for item in res_my_videos['items']:
        tags = item['snippet'].get('tags', [])
        my_tags.extend([t.lower() for t in tags])

    # 3. CALCUL DE LA SYNCHRONISATION
    set_trending = set(trending_tags)
    set_my = set(my_tags)
    common_tags = set_trending.intersection(set_my)

    sync_score = (len(common_tags) / len(set_trending)) * 100 if set_trending else 0

    # 4. AFFICHAGE
    print(f"\n--- AUDIT DES TENDANCES CACH√âES ---")
    print(f"Mots-cl√©s en commun avec les tendances : {list(common_tags)[:10]}...")
    print(f"Indice de Synchronisation Th√©matique : {sync_score:.2f}%")

    # √âchelle de 1 √† 10 pour la sant√© mentale
    mental_impact_score = round(sync_score / 2, 1) # Plus on est synchronis√©, plus l'influence est forte
    if mental_impact_score > 10: mental_impact_score = 10.0

    print(f"Score d'exposition aux th√©matiques globales : {mental_impact_score}/10")

# Appel de la fonction
get_tags_analysis()
```
- Interpr√©tation analytique

Cette analyse r√©v√®le une influence plus diffuse mais plus profonde.
M√™me en l‚Äôabsence de consommation directe des contenus viraux, une forte convergence des th√©matiques indique une exposition aux m√™mes m√©canismes √©motionnels (urgence, conflit, controverse).
Elle met en √©vidence une pollution th√©matique : l‚Äôinfluence de masse persiste sous une forme personnalis√©e, moins visible mais tout aussi structurante pour l‚Äôattention et l‚Äô√©tat cognitif de l‚Äôutilisateur.

# R√©sultats et observations
L‚Äôanalyse met en √©vidence :
- Une forte r√©p√©tition de certains mots-cl√©s
- Une homog√©n√©isation progressive des contenus recommand√©s
- Une exposition diff√©renci√©e selon les centres d‚Äôint√©r√™t initiaux

Ces √©l√©ments confirment l‚Äôexistence de bulles algorithmiques influen√ßant la consommation de contenu.
# Conclusion : Vers une hygi√®ne num√©rique √©clair√©e
Ce projet montre comment un simple audit de donn√©es personnelles peut r√©v√©ler des m√©canismes algorithmiques invisibles.

Gr√¢ce √† :
- l‚Äôacc√®s aux donn√©es personnelles (RGPD),
- l‚Äôusage d‚Äôoutils de programmation accessibles,
- l‚Äôutilisateur peut reprendre une partie du contr√¥le sur sa consommation num√©rique.

L‚Äôobjectif n‚Äôest pas de rejeter YouTube, mais de d√©velopper une conscience critique face aux syst√®mes de recommandation.

# Perspectives d‚Äôam√©lioration
- Analyse temporelle de l‚Äô√©volution des recommandations
- Comparaison entre plusieurs profils utilisateurs
- Visualisations avanc√©es (graphes, r√©seaux de tags)
- Automatisation compl√®te du pipeline

# Auteur
- Nenad Jovanovic
- Thi Hong Nhung Nguyen
