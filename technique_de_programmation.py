# -*- coding: utf-8 -*-
"""Technique de programmation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rtJUV_LdgvJKT15UBLhsACPbhneYEi0G

##Introduction : Le paradoxe de la transparence algorithmique

Nous vivons une √©poque o√π YouTube pr√©tend nous offrir une exp√©rience "sur mesure", adapt√©e √† nos go√ªts personnels. Pourtant, cette personnalisation cache une r√©alit√© inqui√©tante : l'onglet "Tendances", autrefois visible par tous, a progressivement disparu au profit d'un flux algorithmique opaque. Nous avons l'impression de naviguer librement dans un oc√©an de contenus, alors que nous sommes guid√©s par des m√©canismes invisibles qui maximisent notre temps d'√©cran, souvent au d√©triment de notre bien-√™tre psychologique.

**Probl√©matique**

Si les tendances YouTube sont d√©sormais dissimul√©es derri√®re une interface hyper-personnalis√©e, leur influence sur nos comportements a-t-elle r√©ellement disparu ? Comment peut-on mesurer objectivement l'impact de ce flux invisible sur notre sant√© mentale alors m√™me que nous ne percevons plus son existence ? Notre projet r√©pond √† cette question cruciale : en utilisant les droits RGPD et la programmation Python, nous avons construit un outil d'audit qui rend visible l'invisible, r√©v√©lant si nous sommes, √† notre insu, synchronis√©s avec des tendances potentiellement toxiques pour notre √©quilibre mental.

##Code 1 : Installation et configuration initiale

**Explication Technique**

Ce premier code installe l'infrastructure logicielle n√©cessaire au projet. Il importe des biblioth√®ques Python essentielles : Pandas pour la manipulation de donn√©es tabulaires, Matplotlib et Seaborn pour la visualisation, et google-api-python-client pour communiquer avec l'API YouTube. La cl√© API permet d'authentifier les requ√™tes aupr√®s des serveurs Google.

**Interpr√©tation Critique**

Ce code mat√©rialise le principe d'appropriation des donn√©es personnelles. En installant ces outils, vous vous donnez les moyens techniques de "contre-auditer" les plateformes. C'est un acte de r√©sistance num√©rique : au lieu de subir passivement l'algorithme, vous cr√©ez votre propre outil d'analyse. L'utilisation de l'API YouTube est particuli√®rement significative car elle force Google √† vous r√©v√©ler des informations normalement cach√©es dans l'interface utilisateur, comme le classement r√©el des tendances par r√©gion.
"""

# Cellule 1 : Installation des outils
!pip install pandas matplotlib seaborn google-api-python-client isodate

"""##Code 2 : Analyse comportementale profonde

**Explication Technique**

Ce code charge le fichier watch-history.json issu de Google Takeout et enrichit chaque vid√©o avec des m√©tadonn√©es obtenues via l'API (cat√©gorie, dur√©e, nombre de vues, tags). Il impl√©mente un syst√®me de mise √† jour incr√©mentale : seules les nouvelles vid√©os sont analys√©es pour √©conomiser le quota API. Les visualisations produites incluent un graphique des cha√Ænes dominantes, une distribution horaire de visionnage, et une analyse de la popularit√© du contenu consomm√©.

**Interpr√©tation Critique**

Ce script r√©v√®le la "chronobiologie de l'addiction" mentionn√©e dans votre pr√©sentation orale. En cartographiant vos heures de consommation, il expose comment l'algorithme exploite vos moments de vuln√©rabilit√© (soir√©es, fins de semaine). Le graphique des cha√Ænes dominantes mat√©rialise votre "bulle de filtre" : si 80% de votre temps se concentre sur 5-10 cha√Ænes, vous √™tes dans une chambre d'√©cho algorithmique. L'analyse des vues (mainstream vs niche) est particuli√®rement r√©v√©latrice : consommer majoritairement du contenu viral indique une forte exposition aux m√©canismes de comparaison sociale et d'anxi√©t√© d√©crits dans les √©tudes psychologiques.
"""

import pandas as pd
import json
import os
from googleapiclient.discovery import build
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# ==========================================
# CONFIGURATION
# ==========================================
API_KEY = "AIzaSyAWr5iPhQC3U5Af_Ts5bf8qPc6BP_rTJH0" # √Ä ne jamais mettre sur Git ! Utilisez un fichier config ou env.
DB_FILE = "local_video_database.csv"
TAKEOUT_FILE = "watch-history.json" # Le fichier t√©l√©charg√© depuis Google

# ==========================================
# MODULE 1 : GESTION DES DONN√âES ET MISE √Ä JOUR
# ==========================================

def load_takeout_data(filepath):
    """Charge et nettoie les donn√©es brutes de Google Takeout."""
    print(f"Chargement de {filepath}...")
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print("Erreur : Fichier Takeout introuvable.")
        return pd.DataFrame()

    clean_data = []
    for entry in data:
        # On ne garde que les entr√©es qui sont des vid√©os (pas les pubs ou visites de chaine)
        if 'titleUrl' in entry and 'watch?v=' in entry['titleUrl']:
            video_id = entry['titleUrl'].split('watch?v=')[1]
            # Gestion de la date
            watch_time = entry['time'] # Format ISO
            clean_data.append({
                'video_id': video_id,
                'title': entry['title'].replace("Vous avez regard√© ", ""),
                'watch_time': watch_time
            })

    return pd.DataFrame(clean_data)

def get_video_details(video_ids, api_key):
    """Interroge l'API YouTube pour r√©cup√©rer les cat√©gories (Enrichissement)."""
    youtube = build('youtube', 'v3', developerKey=api_key)
    video_details = []

    # L'API accepte max 50 IDs par requ√™te
    chunk_size = 50
    for i in range(0, len(video_ids), chunk_size):
        chunk = video_ids[i:i+chunk_size]
        request = youtube.videos().list(
            part="snippet,contentDetails,statistics",
            id=','.join(chunk)
        )
        response = request.execute()

        for item in response['items']:
            video_details.append({
                'video_id': item['id'],
                'category_id': item['snippet']['categoryId'],
                'channel_title': item['snippet']['channelTitle'],
                'tags': item['snippet'].get('tags', []), # Les tags sont cl√©s pour comprendre les tendances
                'duration': item['contentDetails']['duration'],
                'view_count': item['statistics'].get('viewCount', 0)
            })

    return pd.DataFrame(video_details)

def update_database(new_df):
    """M√©canisme de mise √† jour incr√©mentale."""
    # 1. Charger la base existante si elle existe
    if os.path.exists(DB_FILE):
        existing_db = pd.read_csv(DB_FILE)
        print(f"Base existante charg√©e : {len(existing_db)} vid√©os.")
    else:
        existing_db = pd.DataFrame(columns=['video_id'])
        print("Cr√©ation d'une nouvelle base de donn√©es locale.")

    # 2. Identifier les vid√©os inconnues (qu'on n'a pas encore analys√©es)
    # On compare les IDs du Takeout avec ceux de la DB locale
    merged = new_df.merge(existing_db, on='video_id', how='left', indicator=True)
    unknown_videos = merged[merged['_merge'] == 'left_only']['video_id'].unique()

    print(f"Nouvelles vid√©os d√©tect√©es √† analyser via API : {len(unknown_videos)}")

    if len(unknown_videos) > 0:
        # Attention aux quotas API (limitez si n√©cessaire pour les tests)
        # Pour l'exemple, on limite √† 200 nouvelles vid√©os pour ne pas griller le quota gratuit
        details_df = get_video_details(list(unknown_videos)[:200], API_KEY)

        # Fusionner et sauvegarder
        full_db = pd.concat([existing_db, details_df], ignore_index=True).drop_duplicates(subset=['video_id'])
        full_db.to_csv(DB_FILE, index=False)
        print("Base de donn√©es mise √† jour avec succ√®s.")
        return full_db
    else:
        print("Aucune nouvelle donn√©e API n√©cessaire.")
        return existing_db

# ==========================================
# MODULE 2 : ANALYSE ET VISUALISATION
# ==========================================

def analyze_trends(history_df, metadata_df):
    """Cr√©e les graphiques pour la pr√©sentation."""

    # Fusionner l'historique (QUAND j'ai regard√©) avec les m√©tadonn√©es (QUOI)
    df = history_df.merge(metadata_df, on='video_id', how='inner')

    # Convertir le temps
    df['watch_time'] = pd.to_datetime(df['watch_time'], format='ISO8601') # Added format='ISO8601'
    df['hour'] = df['watch_time'].dt.hour
    df['day_of_week'] = df['watch_time'].dt.day_name()

    # Configuration du style
    sns.set_theme(style="whitegrid")

    # --- Graphique 1 : Ma "Bulle" (Cat√©gories) ---
    # Note: L'API renvoie des ID de cat√©gories (ex: '10' = Music).
    # Il faudrait une map compl√®te, ici simplifi√©.
    plt.figure(figsize=(10, 6))
    top_channels = df['channel_title'].value_counts().head(10)
    sns.barplot(x=top_channels.values, y=top_channels.index, palette="viridis")
    plt.title("Les 10 cha√Ænes qui dominent mon flux (Ma Bulle)")
    plt.xlabel("Nombre de vues")
    plt.tight_layout()
    plt.show()

    # --- Graphique 2 : Chronobiologie de l'addiction ---
    plt.figure(figsize=(10, 6))
    sns.histplot(df['hour'], bins=24, kde=True, color="red")
    plt.title("√Ä quelle heure l'algorithme me capte-t-il le mieux ?")
    plt.xlabel("Heure de la journ√©e (0-24)")
    plt.ylabel("Fr√©quence")
    plt.tight_layout()
    plt.show()

    # --- Graphique 3 : Mainstream vs Niche ---
    # Analyse bas√©e sur le nombre de vues global des vid√©os regard√©es
    df['view_count'] = pd.to_numeric(df['view_count'])

    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df['view_count'])
    plt.xscale('log') # √âchelle logarithmique car les vues varient √©norm√©ment
    plt.title("Distribution des vues : Suis-je un consommateur de contenu 'Viral' ?")
    plt.xlabel("Vues totales de la vid√©o (√âchelle Log)")
    plt.tight_layout()
    plt.show()

# ==========================================
# MAIN
# ==========================================

if __name__ == "__main__":
    # 1. Charger l'historique brut
    history = load_takeout_data(TAKEOUT_FILE)

    if not history.empty:
        # 2. Mettre √† jour les m√©tadonn√©es (API)
        metadata = update_database(history)

        # 3. Lancer l'analyse
        analyze_trends(history, metadata)
    else:
        print("Veuillez placer le fichier watch-history.json dans le dossier.")

"""##Code 3 : Audit d'influence directe (Score de Sensibilit√©)

**Explication Technique**

Ce code r√©cup√®re le Top 50 des vid√©os tendances en France via l'API, puis calcule l'intersection avec votre historique personnel. Il g√©n√®re deux diagrammes circulaires comparant les cat√©gories de contenu tendance versus vos cat√©gories personnelles. Le "CATEGORY_MAP" traduit les identifiants num√©riques YouTube en noms lisibles (Music, Gaming, Education, etc.).

**Interpr√©tation Critique**

C'est le c≈ìur de notre probl√©matique : "Les tendances YouTube ont-elles cess√© de nous influencer si elles sont cach√©es ?". Un score de 0% (comme dans vos r√©sultats) prouve que vous ne regardez aucune vid√©o du Top 50 actuel. Paradoxalement, cela ne signifie PAS que vous √™tes libre de l'influence algorithmique, mais plut√¥t que l'algorithme vous a hyper-personnalis√© au point de cr√©er une bulle √©tanche. Comme le pr√©cise votre oral, cette "niche" n'est pas protectrice : elle vous enferme dans un flux pr√©visible qui maximise votre engagement sans diversit√© cognitive. Les graphiques de cat√©gories r√©v√®lent si votre di√®te num√©rique est d√©s√©quilibr√©e (ex: 70% Entertainment, 5% Education).

"""

import pandas as pd
import json
from googleapiclient.discovery import build
from tabulate import tabulate
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# CONFIGURATION
# ==========================================
API_KEY = "AIzaSyAWr5iPhQC3U5Af_Ts5bf8qPc6BP_rTJH0" # Collez votre cl√© ici
TAKEOUT_FILE = "watch-history.json"
# Liste de correspondance des ID et noms de cat√©gories (les plus courants)
CATEGORY_MAP = {
    "1": "Film & Animation",
    "2": "Autos & Vehicles",
    "10": "Music",
    "15": "Pets & Animals",
    "17": "Sports",
    "19": "Travel & Events",
    "20": "Gaming",
    "22": "People & Blogs",
    "23": "Comedy",
    "24": "Entertainment",
    "25": "News & Politics",
    "26": "Howto & Style",
    "27": "Education",
    "28": "Science & Technology",
    "29": "Nonprofits & Activism",
    "30": "Movies",
    "43": "Shows"
}

# ==========================================
# NOUVELLE FONCTION : AFFICHAGE DES CAT√âGORIES
# ==========================================

def display_category_table():
    """Affiche le tableau des ID et des noms pour la r√©f√©rence."""
    print("\n--- TABLEAU DE R√âF√âRENCE DES CAT√âGORIES YOUTUBE ---")
    table_data = [[id, name] for id, name in CATEGORY_MAP.items()]
    # Utilise tabulate pour un affichage propre
    print(tabulate(table_data, headers=["ID", "Cat√©gorie"], tablefmt="fancy_grid"))

def get_category_name(category_id):
    """Convertit un ID en nom de cat√©gorie."""
    return CATEGORY_MAP.get(category_id, f"ID Inconnu ({category_id})")

# ==========================================
# FONCTIONS D'ANALYSE (Reprise de la version pr√©c√©dente)
# ==========================================

def get_youtube_client():
    return build('youtube', 'v3', developerKey=API_KEY)

def get_current_trending_videos(youtube):
    """R√©cup√®re les tendances actuelles."""
    request = youtube.videos().list(
        part="snippet,statistics",
        chart="mostPopular",
        regionCode="FR",
        maxResults=50
    )
    response = request.execute()

    trending_list = []
    for item in response['items']:
        trending_list.append({
            'video_id': item['id'],
            'category_id': item['snippet']['categoryId']
        })
    return pd.DataFrame(trending_list)

def load_my_history(filepath):
    """Charge et nettoie l'historique."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"Erreur : Fichier {filepath} introuvable.")
        return pd.DataFrame()

    my_history = []
    for entry in data:
        if 'titleUrl' in entry and 'watch?v=' in entry['titleUrl']:
            video_id = entry['titleUrl'].split('watch?v=')[-1][:11]
            my_history.append({'video_id': video_id, 'date': entry['time']})
    return pd.DataFrame(my_history).drop_duplicates(subset=['video_id'])

def analyze_influence():
    youtube = get_youtube_client()

    df_trending = get_current_trending_videos(youtube)
    df_my_history = load_my_history(TAKEOUT_FILE)

    # √âtape 1 : Calculer l'intersection
    videos_communes = df_my_history[df_my_history['video_id'].isin(df_trending['video_id'])]
    nb_communs = len(videos_communes)

    # √âtape 2 : R√©cup√©rer les cat√©gories de votre historique r√©cent
    recent_ids = df_my_history['video_id'].head(50).tolist()
    res = youtube.videos().list(part="snippet", id=",".join(recent_ids)).execute()
    my_categories_list = [item['snippet']['categoryId'] for item in res['items']]

    df_my_cats = pd.DataFrame(my_categories_list, columns=['category_id'])

    # --- Pr√©paration pour le graphique ---
    df_trending['category_name'] = df_trending['category_id'].apply(get_category_name)
    df_my_cats['category_name'] = df_my_cats['category_id'].apply(get_category_name)

    # VISUALISATION
    sns.set_theme(style="whitegrid")

    plt.figure(figsize=(14, 7))

    # Graphique 1 : Cat√©gories en Tendance (Noms affich√©s)
    plt.subplot(1, 2, 1)
    # Utilisez 'category_name' pour les √©tiquettes
    df_trending['category_name'].value_counts().plot(kind='pie', autopct='%1.1f%%', title="Cat√©gories en Tendance (France)")
    plt.ylabel('Proportion')

    # Graphique 2 : Mes Cat√©gories (Noms affich√©s)
    plt.subplot(1, 2, 2)
    df_my_cats['category_name'].value_counts().plot(kind='pie', autopct='%1.1f%%', title="Mes Cat√©gories (Audit Personnel)")
    plt.ylabel('Proportion')

    plt.tight_layout()
    plt.show()

    # CONCLUSION
    print(f"\n--- R√âSULTAT DE L'AUDIT ---")
    print(f"Vous avez regard√© {nb_communs} vid√©os qui sont actuellement dans le Top 50 Tendances.")
    influence_score = (nb_communs / 50) * 100
    print(f"Votre indice d'influence directe est de : {influence_score:.1f}%")

# ==========================================
# MAIN EXECUTION
# ==========================================

if __name__ == "__main__":
    display_category_table() # Affiche le tableau de r√©f√©rence au d√©but
    analyze_influence()

"""##Code 4 : Calcul du Score de Sensibilit√© (1-10)

**Explication Technique**

Ce code simplifie l'output pour produire un diagnostic binaire clair. Il calcule le pourcentage d'influence directe, puis le convertit en score sur 10 (formule : influence% / 5). Il ajoute une couche d'interpr√©tation automatis√©e selon trois paliers : Bulle de Niche (‚â§2), Influence Mod√©r√©e (3-5), Mainstream (‚â•6).

**Interpr√©tation Critique**

Votre score de 1.0/10 avec l'interpr√©tation "Bulle de Niche" illustre parfaitement le paradoxe que vous soulevez : l'algorithme vous conna√Æt "trop bien". Ce n'est pas une victoire, c'est une prison dor√©e. Les √©tudes montrent que l'hyper-personnalisation r√©duit la s√©rendipit√© et l'exposition √† des perspectives diverses, cr√©ant une forme d'isolement cognitif. Le message "L'algorithme vous conna√Æt trop bien pour vous proposer du contenu grand public" est ironique : cela signifie que vous √™tes devenu un profil "rentable" via une pr√©dictibilit√© maximale. Votre comportement est si bien mod√©lis√© que l'IA n'a plus besoin de tester de nouveaux contenus sur vous.

"""

import pandas as pd
import json
from googleapiclient.discovery import build
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# CONFIGURATION
# ==========================================
API_KEY = "AIzaSyAWr5iPhQC3U5Af_Ts5bf8qPc6BP_rTJH0"  # <--- METTEZ VOTRE CL√â ICI
TAKEOUT_FILE = "watch-history.json"

# ==========================================
# FONCTIONS TECHNIQUES
# ==========================================

def calculate_sensitivity_score(influence_percentage):
    """Calcule le score de 1 √† 10."""
    score = influence_percentage / 5
    if score < 1: score = 1.0
    if score > 10: score = 10.0
    return round(score, 1)

def get_current_trending_videos(youtube):
    """R√©cup√®re le Top 50 YouTube France."""
    try:
        request = youtube.videos().list(
            part="snippet,statistics",
            chart="mostPopular",
            regionCode="FR",
            maxResults=50
        )
        response = request.execute()
        trending_list = [{'video_id': item['id'], 'category_id': item['snippet']['categoryId']} for item in response['items']]
        return pd.DataFrame(trending_list)
    except Exception as e:
        print(f"Erreur API YouTube : {e}")
        return pd.DataFrame()

def load_my_history(filepath):
    """Charge votre fichier Takeout."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        my_history = []
        for entry in data:
            if 'titleUrl' in entry:
                video_id = entry['titleUrl'].split('v=')[-1][:11]
                my_history.append({'video_id': video_id})
        return pd.DataFrame(my_history)
    except FileNotFoundError:
        print(f"‚ùå ERREUR : Le fichier '{filepath}' est introuvable. Glissez-le dans le dossier √† gauche sur Colab.")
        return pd.DataFrame()

# ==========================================
# FONCTION PRINCIPALE (Celle qui affiche tout)
# ==========================================

def analyze_influence():
    print("üöÄ D√©marrage de l'analyse...\n")

    # 1. Connexion API
    youtube = build('youtube', 'v3', developerKey=API_KEY)

    # 2. Chargement des donn√©es
    df_trending = get_current_trending_videos(youtube)
    df_my_history = load_my_history(TAKEOUT_FILE)

    if df_trending.empty or df_my_history.empty:
        print("‚ö†Ô∏è Analyse impossible : donn√©es manquantes.")
        return

    # 3. Comparaison (Intersection)
    videos_communes = df_my_history[df_my_history['video_id'].isin(df_trending['video_id'])]
    nb_communs = len(videos_communes)

    # 4. Calcul des scores
    influence_perc = (nb_communs / 50) * 100
    sensi_score = calculate_sensitivity_score(influence_perc)

    # 5. AFFICHAGE DES R√âSULTATS DANS LA CONSOLE
    print("-" * 40)
    print("üìä R√âSULTATS DE L'AUDIT RGPD")
    print("-" * 40)
    print(f"‚úÖ Vid√©os analys√©es dans votre historique : {len(df_my_history)}")
    print(f"üî• Vid√©os en commun avec le Top 50 Tendances : {nb_communs}")
    print(f"üéØ Indice d'influence directe : {influence_perc}%")
    print(f"\n‚ö° SCORE DE SENSIBILIT√â (1-10) : {sensi_score}/10")
    print("-" * 40)

    # 6. Petit message d'interpr√©tation automatique
    if sensi_score <= 2:
        print("Interpr√©tation : Vous √™tes dans une 'Bulle de Niche'. L'algorithme vous conna√Æt trop bien pour vous proposer du contenu g√©n√©raliste.")
    elif sensi_score <= 5:
        print("Interpr√©tation : Influence mod√©r√©e. Vous suivez quelques tendances mais gardez un profil sp√©cifique.")
    else:
        print("Interpr√©tation : Profil 'Mainstream'. Vous √™tes fortement synchronis√© avec la culture populaire actuelle.")

# ==========================================
# LANCEMENT (Indispensable pour que √ßa s'affiche !)
# ==========================================
if __name__ == "__main__":
    analyze_influence()

"""##Code 5 : Analyse s√©mantique des tags (Influence Th√©matique)

**Explication Technique**

Ce code extrait les tags (mots-cl√©s) des vid√©os tendances ET de votre historique r√©cent (20 derni√®res vid√©os). Il calcule l'intersection entre ces deux ensembles de tags pour mesurer une "synchronisation th√©matique". Contrairement au Code 3 qui compare des vid√©os identiques, celui-ci d√©tecte si vous √™tes expos√© aux m√™mes sujets que la masse, m√™me via des vid√©os diff√©rentes.

**Interpr√©tation Critique**

C'est l'analyse la plus subtile et dangereuse pour la sant√© mentale. M√™me avec 0% d'influence directe, vous pouvez avoir 30-40% de synchronisation th√©matique. Cela signifie que bien que vous ne regardiez pas les vid√©os virales elles-m√™mes, vous consommez des contenus construits autour des m√™mes triggers √©motionnels : "clash", "drama", "urgent", "choc". Ce code r√©v√®le ce que votre oral appelle la "pollution th√©matique" : l'algorithme vous sert la toxicit√© virale sous une forme personnalis√©e. Si vos tags communs incluent beaucoup de termes anxiog√®nes ou conflictuels, cela indique que votre bulle, bien que diff√©rente du mainstream, reproduit les m√™mes m√©canismes d'engagement par le stress. C'est la preuve ultime que l'influence de masse existe toujours, elle est juste devenue invisible et insidieuse.
"""

import pandas as pd
from googleapiclient.discovery import build
import matplotlib.pyplot as plt

# CONFIGURATION
API_KEY = "AIzaSyAWr5iPhQC3U5Af_Ts5bf8qPc6BP_rTJH0"
TAKEOUT_FILE = "watch-history.json"

def get_tags_analysis():
    youtube = build('youtube', 'v3', developerKey=API_KEY)

    # 1. R√âCUP√âRER LES TAGS DES TENDANCES (L'air du temps)
    print("Analyse des th√©matiques tendances en France...")
    request = youtube.videos().list(part="snippet", chart="mostPopular", regionCode="FR", maxResults=50)
    res_trending = request.execute()

    trending_tags = []
    for item in res_trending['items']:
        tags = item['snippet'].get('tags', [])
        trending_tags.extend([t.lower() for t in tags])

    # 2. R√âCUP√âRER LES TAGS DE VOTRE HISTORIQUE (Votre bulle)
    print("Analyse de vos th√©matiques personnelles...")
    # On prend les 20 derni√®res vid√©os pour voir l'influence r√©cente
    df_history = load_my_history(TAKEOUT_FILE).head(20)
    my_ids = df_history['video_id'].tolist()

    res_my_videos = youtube.videos().list(part="snippet", id=",".join(my_ids)).execute()
    my_tags = []
    for item in res_my_videos['items']:
        tags = item['snippet'].get('tags', [])
        my_tags.extend([t.lower() for t in tags])

    # 3. CALCUL DE LA SYNCHRONISATION
    set_trending = set(trending_tags)
    set_my = set(my_tags)
    common_tags = set_trending.intersection(set_my)

    sync_score = (len(common_tags) / len(set_trending)) * 100 if set_trending else 0

    # 4. AFFICHAGE
    print(f"\n--- AUDIT DES TENDANCES CACH√âES ---")
    print(f"Mots-cl√©s en commun avec les tendances : {list(common_tags)[:10]}...")
    print(f"Indice de Synchronisation Th√©matique : {sync_score:.2f}%")

    # √âchelle de 1 √† 10 pour la sant√© mentale
    mental_impact_score = round(sync_score / 2, 1) # Plus on est synchronis√©, plus l'influence est forte
    if mental_impact_score > 10: mental_impact_score = 10.0

    print(f"Score d'exposition aux th√©matiques globales : {mental_impact_score}/10")

# Appel de la fonction
get_tags_analysis()

"""##Conclusion : Vers une hygi√®ne num√©rique √©clair√©e

Ces cinq codes forment un syst√®me d'audit progressif qui d√©voile les m√©canismes cach√©s de l'influence algorithmique. Notre projet d√©montre scientifiquement que la disparition de l'onglet Tendances n'a pas supprim√© l'influence de masse, elle l'a simplement rendue plus sophistiqu√©e et imperceptible. L'algorithme ne nous prot√®ge pas des contenus toxiques ou anxiog√®nes ; il nous les administre sous une forme personnalis√©e, cr√©ant l'illusion du choix tout en maintenant son emprise sur notre attention.

Les r√©sultats obtenus r√©v√®lent un double pi√®ge : d'un c√¥t√©, l'hyper-personnalisation nous enferme dans des bulles de niche qui appauvrissent notre diversit√© cognitive ; de l'autre, la synchronisation th√©matique nous expose aux m√™mes m√©canismes viraux (sensationnel, pol√©mique, urgence) que le reste de la population, mais de mani√®re invisible.

Gr√¢ce √† l'usage citoyen du RGPD et aux outils de programmation Python, nous avons pu mat√©rialiser cette influence invisible et la quantifier. Ce diagnostic est la premi√®re √©tape vers la rem√©diation : comprendre que l'on est manipul√© permet d'agir. En identifiant les heures de vuln√©rabilit√©, les cha√Ænes qui dominent notre flux, et les th√©matiques anxiog√®nes qui nous sont servies, nous pouvons reprendre le contr√¥le de notre consommation num√©rique.

L'enjeu n'est pas de rejeter YouTube, mais de d√©velopper une conscience critique face aux m√©canismes d'engagement. Utiliser les fonctions "Pas int√©ress√©", diversifier activement nos sources, et consulter r√©guli√®rement notre audit personnel sont autant de gestes d'hygi√®ne num√©rique essentiels pour pr√©server notre sant√© mentale face √† la logique du "tout viral". Ce projet n'est pas une fin en soi, c'est un outil d'√©mancipation qui permet √† chaque utilisateur de devenir acteur de sa propre protection face aux algorithmes.
"""

!jupyter nbconvert --to html *.ipynb